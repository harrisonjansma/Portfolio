{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Document Classifiers\n",
    "\n",
    "To start out the modelling process, we will create two linear classifiers with a bag of words approach. Note here that linear models are generally used with bag of words because the feature vectors are so sparse. We start with a linear regression paired with TFIDF Vectorization.\n",
    "\n",
    "## TFIDF Bag-of-Words + Logistic Regression\n",
    "\n",
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Training Data and Class Titles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n"
     ]
    }
   ],
   "source": [
    "docs = pd.read_csv('C:\\\\Users\\\\harri\\\\.kaggle\\\\competitions\\\\jigsaw-toxic-comment-classification-challenge\\\\train.csv\\\\train.csv')\n",
    "X = docs.iloc[:,1]\n",
    "y = docs.iloc[:,2:]\n",
    "class_names = list(docs.columns)[2:]\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing TFIDF \n",
    "\n",
    "Here we convert our documents to arrays that encode the number of times each word in our corpus is used in each particular document. We then down weight more frequent words as they are less useful for discerning particular classes of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(strip_accents = 'unicode',\n",
    "                        analyzer = 'word',\n",
    "                        stop_words = 'english',\n",
    "                        max_features = 10000 )\n",
    "X_tfidf = tfidf.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting a Logistic Regression for Each Document Class Type\n",
    "\n",
    "Here we take a One-vs-All approach to classification by fitting a logistic regression to each document class. Our model will essentially be given a class, for example \"Toxic\", and then be fed documents and asked to decide whether each is \"Toxic\" or \"Not-Toxic\". Below we see a scoring metric for each class of document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic 0.9665584030848896\n",
      "severe_toxic 0.9820968963740692\n",
      "obscene 0.9769778379928169\n",
      "threat 0.9759224558498725\n",
      "insult 0.9724199291295372\n",
      "identity_hate 0.9714379317504337\n"
     ]
    }
   ],
   "source": [
    "CV_scores = []\n",
    "for name in class_names:\n",
    "    y = docs[name]\n",
    "    clf = LogisticRegressionCV()\n",
    "    score = np.mean(cross_val_score(clf, X_tfidf, y, cv = 5, scoring = 'roc_auc', n_jobs = -1))\n",
    "    print(name, score)\n",
    "    CV_scores.append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Average Score of all Categories: 0.974"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9742355756969365"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(CV_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is OK for a baseline, we scored on  the 30th percentile based on the above cross validation. However if we want to score in the top 25% we need a ROC-AUC score of  0.9858. Next lets try a Naive Bayes-Support Vector Machine Approach.\n",
    "\n",
    "## NB-SVM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
